สรุปการใช้ Media Pipe และ MMAction2 สำหรับ Action Recognition 

สรุปความคืบหน้าการใช้ MediaPipe ร่วมกับการตรวจจับพฤติกรรม
ในระยะเริ่มต้นของการพัฒนา ได้ดำเนินการใช้ MediaPipe Pose ซึ่งเป็นเครื่องมือจาก Google สำหรับการตรวจจับจุดพิกัดของร่างกายมนุษย์ (pose landmarks) โดยสามารถดึงข้อมูลจุดร่างกายได้ทั้งหมด 33 จุดต่อเฟรม จากภาพกล้องแบบเรียลไทม์

จากข้อมูลดังกล่าว ได้นำมาวิเคราะห์เพื่อแยกแยะพฤติกรรมของบุคคลผ่าน กฎเชิงเงื่อนไข (rule-based logic) โดยสามารถจำแนกพฤติกรรมเบื้องต้นได้ 3 ประเภท ได้แก่
•	การยกของ (Lifting): ตรวจจากตำแหน่งข้อมือที่ยกสูงกว่าสะโพกและอยู่ใกล้ลำตัว
•	การยืนนิ่ง (Standing): ตรวจจากมือที่อยู่ต่ำกว่าสะโพก และสะโพกไม่มีการเคลื่อนที่มาก
•	การเคลื่อนไหว (Moving): ตรวจจากตำแหน่งสะโพกที่มีการเปลี่ยนแปลงต่อเนื่องในเฟรมต่างๆ
ได้มีการพัฒนาโค้ดให้สามารถ ติดตามเฉพาะบุคคลที่อยู่ในสถานะ "ยกของ" โดยใช้ตำแหน่งของสะโพก (hip coordinate) เป็น anchor point เพื่อใช้ในการ tracking ต่อเนื่องข้ามหลายเฟรม

การทดลองใช้ MMAction2 สำหรับวิเคราะห์พฤติกรรม
ได้มีการทดลองใช้งาน MMAction2 ซึ่งเป็นเฟรมเวิร์กสำหรับ action recognition บนวิดีโอ โดยเลือกใช้โมเดลมาตรฐาน 2 ตัว ได้แก่:
•	TSN (Temporal Segment Networks): เป็นโมเดลแบบ 2D CNN ที่ประมวลผลข้อมูลเป็นภาพเฟรมแยก ไม่ใช้ลำดับของเฟรมเชิงลึก
•	SlowFast: เป็นโมเดลแบบ 3D CNN ที่ประมวลผลวิดีโอในเชิงเวลา โดยมีการใช้ stream ความเร็วต่างกันในการเรียนรู้ทั้งข้อมูลเคลื่อนไหวเร็วและช้า

จากการทดลอง พบว่า:
•	TSN สามารถทำนายพฤติกรรมพื้นฐานได้ระดับหนึ่ง และมีข้อดีคือ ใช้ทรัพยากรน้อยกว่า ทำให้สามารถ inference ได้รวดเร็ว เหมาะสำหรับงานเบาๆ
•	SlowFast มีประสิทธิภาพสูงกว่าในการเข้าใจลักษณะการเคลื่อนไหวที่ซับซ้อน เช่น การยกของ แต่ ใช้ทรัพยากรมากกว่าอย่างชัดเจน

อย่างไรก็ตาม ทั้งสองโมเดลยังไม่ได้ถูกเทรนเฉพาะกับพฤติกรรม “การยกของ” ที่ต้องการ จึงพบว่า ผลการพยากรณ์ยังไม่ตรงกับวัตถุประสงค์โดยตรง และจำเป็นต้องมีการ fine-tune โมเดลเพิ่มเติมด้วยข้อมูลเฉพาะของโจทย์ เพื่อให้ได้ผลลัพธ์ที่แม่นยำยิ่งขึ้นในบริบทจริง

แผนพัฒนาต่อเนื่อง
หลังจากทดลองใช้ MMAction2 ร่วมกับโมเดล TSN และ SlowFast สำหรับการตรวจจับพฤติกรรม พบว่าแม้จะสามารถพยากรณ์พฤติกรรมพื้นฐานได้บางส่วน แต่ยังไม่ตอบโจทย์เฉพาะด้าน เช่น การตรวจจับ “การยกของ” ได้อย่างแม่นยำโดยตรง 
อีกทั้งยังต้องใช้ทรัพยากรระบบสูง โดยเฉพาะในกรณีของ SlowFast ที่เป็น 3D CNN ซึ่งต้องประมวลผลข้อมูลวิดีโอแบบต่อเนื่องหลายเฟรม

เพื่อให้ระบบมีประสิทธิภาพสูงขึ้นทั้งด้านความแม่นยำและความเบาในการประมวลผล จึงวางแผนที่จะเปลี่ยนมาใช้วิธีการที่ผสมผสานระหว่าง MediaPipe และ LSTM Model โดยมีเหตุผลหลักดังนี้:
•	MediaPipe Pose สามารถดึงจุดพิกัดของร่างกาย (pose landmarks) ได้แบบเรียลไทม์ ด้วยความเร็วสูงและใช้ทรัพยากรต่ำ
•	การนำ landmarks ที่ได้มาเรียงเป็นลำดับเวลา แล้วส่งเข้า LSTM (Long Short-Term Memory) ซึ่งเหมาะกับข้อมูลลำดับเชิงเวลา จะช่วยให้โมเดลเรียนรู้รูปแบบการเคลื่อนไหวได้แม่นยำมากขึ้น
•	วิธีนี้ช่วยลดขนาดข้อมูล ลดภาระของ GPU ในการประมวลผลภาพทั้งเฟรม เหลือเพียงข้อมูลจุดสำคัญของร่างกายเท่านั้น
•	เหมาะสมกับกรณีการตรวจจับพฤติกรรมเฉพาะทาง เช่น “การยกของ” “การเดิน” หรือ “การยืนนิ่ง” ที่มีพื้นฐานจากลักษณะของท่าทางโดยตรง
ด้วยเหตุนี้ จึงตัดสินใจเลือกแนวทาง MediaPipe + LSTM เป็นแนวทางหลักในการพัฒนาระบบตรวจจับพฤติกรรมในระยะต่อไป
